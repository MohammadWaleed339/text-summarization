# Text Summarization Project ‚ú®

This project explores and compares various transformer models for the task of text summarization, aiming to generate concise and informative summaries from longer text inputs.

## Models Used ü§ñ

*   **GPT-2 Medium:** A generative pre-trained transformer used for text generation, adapted for summarization through prompting.
*   **T5 Base:** A Text-to-Text Transfer Transformer model well-suited for various text tasks, including summarization.
*   **BART Large CNN:** A powerful denoising autoencoder model commonly used for summarization.
*   **PEGASUS CNN/DailyMail:** A model specifically pre-trained on news articles for abstractive summarization.

## Datasets üìÅ

*   **CNN/DailyMail:** A widely used dataset containing news articles and their corresponding summaries for initial model exploration.
*   **Samsum:** A dataset of conversational dialogues and their human-written summaries, used for fine-tuning the BART model to handle spoken-style text.

## Project Steps üöÄ

1.  **Load & explore CNN/DailyMail:** Understand the structure and content of the news article dataset.
2.  **Generate initial summaries:** Use pre-trained models (GPT-2, T5, BART, PEGASUS) to get a first look at their summarization capabilities.
3.  **Analyze Samsum dataset:** Examine the characteristics of the conversational dialogue dataset, like text lengths.
4.  **Prepare Samsum data for training:** Preprocess and tokenize the Samsum dataset to be used for model fine-tuning.
5.  **Fine-tune BART-Large-CNN on Samsum:** Train the BART model on the Samsum dataset for one epoch to specialize it in summarizing dialogues.
6.  **Summarize a custom dialogue:** Use the fine-tuned model to generate a summary for a new, user-provided conversation.

## Results üìà

*   **Model Comparison:** Demonstrated and compared the text summarization outputs of different transformer models.
*   **Dialogue Summarization:** Showed that the BART model can effectively summarize conversational text after fine-tuning on the Samsum dataset.
*   **Example Summaries:** Presented example summaries generated by each model for qualitative comparison.
*   **Custom Summary:** Successfully generated a relevant summary for a user-defined custom dialogue.

## How to Run ‚ñ∂Ô∏è

1.  Clone the repository.
2.  Install necessary libraries: `pip install -U accelerate bertviz umap-learn sentencepiece py7zr datasets transformers torch`
3.  Run the notebook cells sequentially.

## Future Work

*   **Quantitative Evaluation:** Evaluate model performance using metrics like ROUGE scores.
*   **Experimentation:** Try different fine-tuning parameters and training epochs.
*   **Explore More Models:** Investigate and test other transformer models and summarization techniques.
*   **Build UI:** Create a user interface for easy summarization of custom text inputs.
